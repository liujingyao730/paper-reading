# GCN相关的基础文献
图卷积网络是针对于图结构数据的有力分析工具，这四篇文章是图卷积网络领域内比较重要的基础性工作，对于我们理解和使用图卷积网络有着很重要的意义
## 目录
* [before deep learing graph neural network](###图卷积)
* [convolutional neural networks on graphs with fast localized spectral filtering](#图卷积的快速计算)
* [Semi-supervised classification with graph convolutional network-iclr2017](#一阶近似与半监督学习)
* [Deeper insights into graph convolutional networks for semi-supervised learning aaai2018](#deeper-insights)
* [Diffusion convolutional recurrent neural network Data-driven traffic forecasting](#扩散卷积)
### 图卷积
在Euclidean domains(图像，声音信号等类型)的数据中，卷积操作就是将函数进行傅里叶变换映射到频域空间，之后进行相乘再做逆傅里叶变换得到的结果。对于图结构的数据，如果我们想要将卷积领域进行扩展，就需要合理的定义在图领域的傅里叶变换，进而可以定义图领域的卷积操作。  
把Euclidean domains中的傅里叶变换迁移拓展到图领域中，最核心的步骤在于把拉普拉斯算子的特征函数$e^{i\omega t}$在图领域中做出对应的映射，而这个对应的映射就是图拉普拉斯矩阵的特征向量
* 传统领域的傅里叶变换  <div id='label'>
  
> 传统的傅里叶变换定义为：$F(\omega)=\mathcal{F}[f(t)]=\int f(t)e^{i\omega t}dt$，这是信号$f(t)$与基函数$e^{i\omega t}dt$之间的积分，而基函数选择它的原因在于$e^{i\omega t}dt$是拉普拉斯算子的特征函数  
> 同样的，当我们想将卷积拓展到图领域时，因为图的拉普拉斯矩阵就是离散的拉普拉斯算子，所对应的选择的基函数就应当是图拉普拉斯矩阵的特征向量

* 图拉普拉斯矩阵的定义与分解

> 图的拉普拉斯矩阵通常定义为$L=D-A，$其中$D$指顶点度数组成的对角矩阵，$A$指邻接矩阵或者边权重矩阵；在运算中通常采用归一化后的拉普拉斯矩阵$L^{sys} = D^{-1/2}LD^{-1/2}$
> 图拉普拉斯矩阵是对称半正定矩阵，它的特征向量之间相互正交，所有的特征向量构成的矩阵成为正交矩阵，因此我们可以知道拉普拉斯矩阵一定可以进行谱分解，并且分解后有特殊的形式：
> $$L=U \left[ \begin{matrix}\lambda_1 & & \\ & ... & \\ & & \lambda_n\end{matrix}\right]U^T$$
> 其中$U$是由拉普拉斯矩阵特征向量组成的矩阵，而$\lambda$代表着拉普拉斯矩阵的特征值

* 图领域的傅里叶变换
> 仿照传统领域下的傅里叶变换定义，我们就可以得到图领域的傅里叶变换
> $$ F(\lambda_l)=\hat{f}(\lambda_l)=\sum^N_{i=1}f(i)u^*_l(i)$$
> $f$是图上的N维向量，$f(i)$表示节点i上对应的输入，$u_l(i)$表示第$l$个特征向量的第$i$个分量，特征值就对应了在不同基函数下对应的分量，也可以在一定程度上认为是对应的频率。$f$的图傅里叶变换就是与$\lambda_l$对应的特征向量$u_l$进行内积运算
> 进一步的，我们图傅里叶变换的矩阵形式写成：
> $$ \left[ \begin{matrix} 
> \hat f(\lambda_1) \\
> \hat f(\lambda_2) \\
> \vdots\\
> \hat f(\lambda_N) 
> \end{matrix} \right] = 
> \left[ \begin{matrix} 
> u_1(1) & u_1(2) & \cdots & u_1(N) \\
> u_2(2) & u_2(2) & \cdots & u_N(2) \\
> \vdots & \vdots & \ddots & \vdots \\
> u_1(N) & u_2(N) & \cdots & u_N(N)
> \end{matrix} \right]
> \left[\begin{matrix}
> \hat f(\lambda_1) \\
> \hat f(\lambda_2) \\
> \vdots\\
> \hat f(\lambda_N)
> \end{matrix}\right] $$
> 也就是说$\hat{f}=U^Tf$，逆傅里叶变换也可同样的形式推广：$f=U\hat{f}$

* 图卷积
> 对于卷积核$h$，我们将其进行傅里叶变换之后的结果写成对角矩阵的形式就是：$g(\Lambda)=diag(\hat{h}(\lambda_1), \hat{h}(\lambda_2),...,\hat{h}(\lambda_N))$，所以$h与f$的卷积就可以写成：
> $$f*_gh= Ug(\Lambda)U^Tf$$
### 图卷积的快速计算
回顾传统二维图像中的卷积，我们可以发现二维的卷积具备着两个很好的特性：
* 局部连接：每一个卷积核在不同的位置只对这一个位置的局部具备感受的能力，不接收其他区域的信息
* 参数共享：每一个卷积核在不同的位置都使用相同的参数，这就极大的减少了所需学习的参数数量

但是在上述图卷积的操作中这两点都不具备，每一个卷积核所需要学习的参数$g(\Lambda)=diag(\hat{h}(\lambda_1), \hat{h}(\lambda_2),...,\hat{h}(\lambda_N))$的规模为$\mathcal{O}(N)$，$N$表示节点数目，在多通道，多卷积核的情况下参数的数目相当的庞大；另外在这样的操作下就意味着每一个节点都可以看到所有的节点的信息，这样就不具备局部连接的特性  
因此，为了适应深度学习的需求，学者们对图卷积做了一定程度的改变使得它具备了局部感知特性并且降低了参数数量：
#### Polynomial parametrization for localized filters  
多项式参数化就是将原本作为卷积核参数的对角矩阵，由简单的学习对角矩阵对角线上每一个元素改变成学习一个多项式的系数，即：
$$g(\Lambda)=diag(\theta_1, \theta_2,...,\theta_N) \rightarrow g(\Lambda)=\sum_i^{K-1}\theta_i\Lambda^k$$
这样原本的卷积操作就变成：
$$\begin{aligned}
    f *_g h =& Ug(\Lambda)U^Tf\\
    =&U(\sum_i^{K-1}\theta_i\Lambda^k)U^Tf\\
    =&(\sum_i^{K-1}\theta_iL^k)f
\end{aligned}$$
这样就将原本的参数量从$\mathcal{O}(N)$降低到了$\mathcal{O}(K)$，同时由于拉普拉斯矩阵的特殊性，$K$具备着明确的含义，即每一个节点所能看到的节点的最近距离，这样就达到了降低参数量同时使得卷积核具备了局部连接性的目的
#### Recursive formulation for fast filtering
在进行了上述的变化之后，虽然降低了整体的参数量也避免了拉普拉斯矩阵的特征值分解计算，但在实际的计算中计算代价仍然不小（拉普拉斯矩阵的分解步骤可以在训练前就分解完成，整体更新参数时只要调用分解完成的特征向量矩阵即可，而同时因为需要计算拉普拉斯矩阵的乘积，空间存储本身的复杂度就在$\mathcal{O}(N^2)$级别，所以整体避免分解只是略微降低了空间复杂度）  
因此，为了避免$\mathcal{O}(N^3)$的矩阵乘（虽然在算法层面有更快级别的算法），作者提出采用切比雪夫多项式逼近的方法来通过递归计算拉普拉斯矩阵乘向量的方式降低计算的复杂度，即：
$$g(\Lambda)=\sum_i^{K-1}\theta_i\Lambda^i\approx\sum_i^{K-1}\theta_iT_i(\tilde{L})$$
其中$\tilde{L}=2L/\lambda_{max} - I_N$这是为了保证切比雪夫多项式的数学性质而做出的变换，切比雪夫多项式的计算公式为：$T_k(L)=2xT_{k-1}(x)-T_{k-1}(x), T_1(x)=x, T_0(x)=1$，这样，如果我们将$T_i(\tilde{L})f$记做$x_k$的话，卷积操作就变成$f *_g h=\sum_i^{K-1}x_i$，而每一个$x_i$可以通过$x_i = 2\tilde{L}x_{i-1}-x_{i-2}$的方式递归计算得到，而由于拉普拉斯矩阵是一个稀疏矩阵，那么整体的计算复杂度就降低到$\mathcal{O}(\mathcal{E})$，$\mathcal{E}$表示边的数量，这样就大大的降低的计算的复杂度，使得图卷积操作适应了深度学习的要求
#### Fast Pooling of Graph Signals
进一步地，在图卷积中我们可以构建一个类似于池化的操作，把相近似的节点相互聚合起来，这样的操作需要满足两点：1）我们需要可以控制，或者明确知道每一轮类比的池化操作之后图会缩减成什么尺寸；2）类比的池化操作需要可以快速的实现，最好可以容易的并行化计算，这样就可以进行加速。
首先我们需要介绍一下Graclus multilevel clustering算法，这是一个基于贪心规则的算法，具体的步骤是，在未标记的点中选中一个，然后在它未被标记的邻接节点中选取可以最大化$W_{ij}(\frac{1}{d_i}+\frac{1}{d_j})$ 的节点，将这两个节点合并成一个，把他们的与其他节点的边权重加和作为新节点的边权重，重复直至所有的节点都探索，这样整体上图的大小会被缩小一半（会有一些单独节点singleton的存在）
在实际的计算中，为了便于并行化与快速的计算，整体上通过平衡二叉树的方式将节点之间在不同层级的合并关系组织起来，具体来说就是：1）每一层的节点都是前一层合并而来的两个的父节点；2）单子节点(singleton)将会配备一个虚拟节点与它一起作为下一层父节点的兄弟节点，每一个虚拟节点都设置成为中性值(neutral value)，也就是0，这样以ReLU函数作为激活函数同时通过max pooling的方式就不会因为添加了虚拟节点而造成影响：
![fast pooling](../pics/fast_pooling.png)
### 一阶近似与半监督学习
有了前面的基础之后，学者们进一步提出了更加简化的图卷积框架，在上述的图卷积操作$g(\Lambda)x=\sum_i^{K-1}\theta_iT_i(\tilde{L})x$中，如果我们将$K$的值限定为2，在实际卷积中就意味着每个节点只能看到与自己邻接的节点信息，图卷积的操作就会变为：
$$g(\Lambda)x=\theta_0x+\theta_1\tilde{L}x$$
因为神经网络中对数据的尺度变换和归一化操作，我们可以进一步假设$\lambda_{max}\approx2$，再加上这里我们使用的是拉普拉斯矩阵的归一化形式:$L=I_n-D^{-1/2}AD^{-1/2}$那么我们可以将上式进一步简化为：
$$g(\Lambda)x=\theta_0x+\theta_1(L-I_n)x=\theta_0x-D^{-1/2}AD^{-1/2}\theta_1x$$
再进一步地，为了简化参数这里指定$\theta_0=-\theta_1=\theta$上式就简化为:
$$g(\Lambda)x=\theta(I_n+D^{-1/2}AD^{-1/2})x$$
随后文章进行了进一步的化简，为了方便下一节讲述deeper insights，这里就不再进一步化简，详细的化简参见文章内容
有了这样的图卷积操作，就可以进行快速的半监督学习，具体的方法就是当网络最后的输出之后，只对有标记的节点计算交叉熵损失，然后进行梯度回传更新网络参数，输出时预测所有节点类别
### deeper-insights
为什么GCN可以取得很好的效果呢？当我们卷积操作$g(\Lambda)x=\theta(I_n+D^{-1/2}AD^{-1/2})x$聚焦在单一的某个节点时，就会得到：

$$h^r_i=\theta(h^{r-1}_i+\sum_{j\in\mathcal{N}(i)}\frac{h_j^{r-1}}{\sqrt{d_id_j}})$$

这就是另外一种形式的拉普拉斯平滑，这样的平滑本质上的含义在于，每一个节点与自己相邻近节点相近似，每一层的图卷积会让每一个节点与自己相近的节点（也更大可能上是处于同一类别的节点）交换信息，这也就是为什么图卷积网络在半监督学习中会取得较好结果的原因
但是图卷积网络与传统卷积网络不同的一点在于，并不是越深层的卷积层数就会带来更好的效果，事实上，在半监督分类问题中，层数过多会降低整体网络效果：
![卷积网络层数与分类问题](../pics/gcn_layer.png)
这源自于拉普拉斯平滑本身的特性，快速的进行连续几层的拉普拉斯平滑会导致图中联通区域内的节点的值趋于一致。
而这样的特性本身限制了图卷积网络的效果，深层的网络会导致过度平滑而使得连通区域趋于一致，但浅层的网络并不能使节点充分利用到整个图的信息，在这篇文章中作者提出了一种联合训练的方式来避免在深层图卷积网络中带来的过度平滑的问题，同时又可以通过深层的网络将信息传播到整个图中
### 扩散卷积